# -*- coding: utf-8 -*-
"""Multiple Linear Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vDN0m3nsIPaGHbKR4g4JQGgyLwPjm_pB

**MULTIPLE** **LINEAR** **REGRESSION**

Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.linear_model import LinearRegression,Ridge,Lasso
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
from scipy import stats

"""LOAD THE  DATASET"""

df=pd.read_csv('/content/cars.csv')
df.shape

df.head()

df.info()

#numerical_variables
 num_var=df.select_dtypes(include=['int64','float64']).columns
 num_var

#categorical variables
cat_var=df.select_dtypes(include=['object']).columns
cat_var

df.describe()

"""MISSING VALUES"""

print("Missing Values in each Column")
df.isnull().sum()

"""DUPLICATE ROWS"""

dupli = df.duplicated().sum()
print("Number of duplicate rows:",dupli)

#Remove duplicates if any

data = df.drop_duplicates()
print("New dataset dimension after removing duplicates:",data.shape)

"""**Handle** **Categorical** **Variables**"""

data_encoded = pd.get_dummies(data, columns=cat_var,drop_first=True)
print("Shape after encoding categorical variables:",data_encoded.shape)

target_column="Engine Information.Engine Statistics.Horsepower"
X=data_encoded.drop(target_column,axis=1)
y=data_encoded[target_column]

"""**Detect** **Outliers**"""

def detect_outliers(data):
    Q1=data.quantile(0.25)
    Q3=data.quantile(0.75)
    IQR=Q3-Q1
    return ((data < (Q1 -1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).sum()


outliers=detect_outliers(df[num_var])
print(outliers)

# Iterate through numerical columns to handle outliers
for col in num_var:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filter data for the current column
    data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]

print("Data shape after handling outliers:", data.shape)

"""DATA VISUALIZATION"""

#correlation matrix
plt.figure(figsize=(10,8))
correlation_matrix=df[num_var].corr()
sns.heatmap(correlation_matrix,annot=True,cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""STANDARDIZATION"""

#feature scaling
scaler=StandardScaler()
data_encoded[num_var]=scaler.fit_transform(data_encoded[num_var])
data_encoded.head()

#Define independent variables(x)
x = df[["Dimensions.Length", "Dimensions.Width", "Dimensions.Height",
         "Engine Information.Number of Forward Gears", "Fuel Information.City mpg",
         "Fuel Information.Highway mpg", "Engine Information.Engine Statistics.Torque"]] # Split the column names
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x) # Assign the scaled data to a new variable

vif_data = pd.DataFrame() # Create an empty DataFrame
vif_data["Feature"] = x.columns # Use the original DataFrame's columns
vif_data["VIF"] = [variance_inflation_factor(x_scaled, i) for i in range(len(x.columns))] # Use scaled data for VIF calculation
vif_data

#Drop the correlated feature'Fuel Information.Highway mpg'
x = df[["Dimensions.Length", "Dimensions.Width", "Dimensions.Height",
         "Engine Information.Number of Forward Gears", "Fuel Information.City mpg",
         "Fuel Information.Highway mpg", "Engine Information.Engine Statistics.Torque"]]

#Standardize the data
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

#compute new VIF values
vif_data = pd.DataFrame()
vif_data["Feature"] = x.columns
vif_data["VIF"] = [variance_inflation_factor(x_scaled, i) for i in range(x_scaled.shape[1])]
print("updated VIF values:")
print(vif_data)

"""**Build** **and** **Train** **the** **Regression** **Model**"""

from sklearn.model_selection import train_test_split

# Assuming X and y are your feature and target data respectively
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed

# Now you can proceed with model training
model = LinearRegression()
model.fit(X_train, y_train)

"""TRAIN MODEL"""

# Target variable (dependent variable)
target = "Engine Information.Engine Statistics.Horsepower"

# Train Multiple Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Model evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Display results
print("Mean Squared Error (MSE):", mse)
print("R-squared (R²):", r2)

# Display regression equation
coefficients = model.coef_
intercept = model.intercept_

# Get the feature names from the DataFrame used for training
num_features = X_train.columns  # Assuming X_train is a pandas DataFrame

print("\nLinear Regression Equation:")
equation = "Horsepower = {:.2f}".format(intercept)
for feature, coef in zip(num_features, coefficients):
    equation += " + ({:.2f} * {})".format(coef, feature)
print(equation)

"""Actual vs Predicted"""

# Train Multiple Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Model evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Display results
print("Mean Squared Error (MSE):", mse)
print("R-squared (R²):", r2)

# Display regression equation
coefficients = model.coef_
intercept = model.intercept_

# Get the feature names from the DataFrame used for training
num_features = X_train.columns  # Assuming X_train is a pandas DataFrame

print("\nLinear Regression Equation:")
equation = "Horsepower = {:.2f}".format(intercept)
for feature, coef in zip(num_features, coefficients):
    equation += " + ({:.2f} * {})".format(coef, feature)
print(equation)

# Plot Actual vs Predicted values
plt.figure(figsize=(8,6))
sns.scatterplot(x=y_test, y=y_pred)
plt.xlabel("Actual Horsepower")
plt.ylabel("Predicted Horsepower")
plt.title("Actual vs Predicted Horsepower")
plt.show()

# Plot Actual vs Predicted with Regression Line
plt.figure(figsize=(8,6))
sns.regplot(x=y_test, y=y_pred, scatter_kws={"color": "blue"}, line_kws={"color": "red"})  # Red regression line
plt.xlabel("Actual Horsepower")
plt.ylabel("Predicted Horsepower")
plt.title("Actual vs Predicted Horsepower with Regression Line")
plt.show()